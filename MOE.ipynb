{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGPgsiUaRj4h",
        "outputId": "f24ff0b4-b7b1-419a-f1c0-3c65098bbcd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDhIjei3RuCW"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import io\n",
        "import sys\n",
        "import time\n",
        "import urllib.request\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFrpskBFRz7w"
      },
      "outputs": [],
      "source": [
        "class cfg:\n",
        "  use_moe = True # set False for dense baseline\n",
        "  n_layer = 6\n",
        "  n_head = 8\n",
        "  d_model = 512\n",
        "  d_mlp = 2048  # dense MLP hidden\n",
        "  vocab_limit = None # None = use all chars\n",
        "  block_size = 256 #sequence length\n",
        "  batch_size = 24 #tokens per batch = batch_size * block_size\n",
        "  grad_accum_steps = 2 #effective batch = batch_size * grad_accum_steps\n",
        "  max_steps = 400 #quick demo ; increase for better loss\n",
        "  lr = 3e-4\n",
        "  weight_decay = 0.1\n",
        "  warmup_steps = 0.1\n",
        "  compile_model = False #torch.compile may slow first step\n",
        "  dropout = 0.0\n",
        "\n",
        "  #MOE specifics\n",
        "  n_experts = 4\n",
        "  top_k = 1 #switch-style\n",
        "  capacity_factor = 1.25 #per-expert token capacity\n",
        "  load_balance_coef = 0.01\n",
        "  zloss_coef = 0.001\n",
        "\n",
        "  #precision + device\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n",
        "  seed = 42\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pK8qOI0Tz9H"
      },
      "outputs": [],
      "source": [
        "cfg = cfg()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-2wSIM2T2Hs",
        "outputId": "cb8c51b3-302f-465e-e33f-93fdfd54aeb1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7948929bfcb0>"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(cfg.seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCs12aoYT5Nl"
      },
      "outputs": [],
      "source": [
        "torch.cuda.manual_seed_all(cfg.seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gl_kiOJ3T_WW"
      },
      "outputs": [],
      "source": [
        "def load_tinyshakespeare():\n",
        "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "    try:\n",
        "        txt = urllib.request.urlopen(url, timeout=10).read().decode(\"utf-8\")\n",
        "    except Exception:\n",
        "        # Fallback tiny corpus\n",
        "        txt = (\n",
        "            \"To be, or not to be, that is the question:\\n\"\n",
        "            \"Whether 'tis nobler in the mind to suffer\\n\"\n",
        "            \"The slings and arrows of outrageous fortune,\\n\"\n",
        "            \"Or to take arms against a sea of troubles\\n\"\n",
        "            \"And by opposing end them.\\n\"\n",
        "        ) * 200\n",
        "    return txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzMfMOP1UGDo"
      },
      "outputs": [],
      "source": [
        "text = load_tinyshakespeare()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdQGhpWXUH6L"
      },
      "outputs": [],
      "source": [
        "chars = sorted(list(set(text)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yz6kzOK3Zd0A",
        "outputId": "bba8472e-16a7-4631-8379-f910abd2088d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['\\n',\n",
              " ' ',\n",
              " '!',\n",
              " '$',\n",
              " '&',\n",
              " \"'\",\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '3',\n",
              " ':',\n",
              " ';',\n",
              " '?',\n",
              " 'A',\n",
              " 'B',\n",
              " 'C',\n",
              " 'D',\n",
              " 'E',\n",
              " 'F',\n",
              " 'G',\n",
              " 'H',\n",
              " 'I',\n",
              " 'J',\n",
              " 'K',\n",
              " 'L',\n",
              " 'M',\n",
              " 'N',\n",
              " 'O',\n",
              " 'P',\n",
              " 'Q',\n",
              " 'R',\n",
              " 'S',\n",
              " 'T',\n",
              " 'U',\n",
              " 'V',\n",
              " 'W',\n",
              " 'X',\n",
              " 'Y',\n",
              " 'Z',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z']"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTtWn7RkZff6"
      },
      "outputs": [],
      "source": [
        "if cfg.vocab_limit :\n",
        "  chars = chars[: cfg.vocab_limit]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uh1uYsymdiP0"
      },
      "outputs": [],
      "source": [
        "stoi = {ch : i for i ,ch in enumerate(chars)}\n",
        "\n",
        "itos = { i : ch for ch,i in stoi.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VSBxgfadx21"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ks_IDjbkd0aF",
        "outputId": "e1920240-0455-49fe-8b8a-132f53f52314"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPlVX2Yrd1cK"
      },
      "outputs": [],
      "source": [
        "def encode(s):\n",
        "  return torch.tensor([stoi[c] for c in s if c in stoi] , dtype = torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ct8Xlg41eCBN"
      },
      "outputs": [],
      "source": [
        "def decode(t):\n",
        "  return ''.join([itos[int(i)] for i in t])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_7xS7-WeMc8"
      },
      "outputs": [],
      "source": [
        "data = encode(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2BO7NCAeO99",
        "outputId": "74826559-cf1b-407c-ae37-b82c33447a1d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([18, 47, 56,  ..., 45,  8,  0])"
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeCzt2s6ePuG"
      },
      "outputs": [],
      "source": [
        "n = int(0.9 * len(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_Zz8ITgeSXd",
        "outputId": "1d3d1357-ece8-42a7-fcf7-5efe796569b3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1003854"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpF3CAhceU91"
      },
      "outputs": [],
      "source": [
        "train_data , val_data = data[:n],data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8-j0hwUebNt"
      },
      "outputs": [],
      "source": [
        "def get_batch(split):\n",
        "  d = train_data if split == \"train\" else val_data\n",
        "  ix = torch.randint(len(d) - cfg.block_size -1 , (cfg.block_size,))\n",
        "  x = torch.stack([d[i : i+cfg.block_size] for i in ix])\n",
        "  y = torch.stack([d[i+1 : i+1+cfg.block_size] for i in ix])\n",
        "  return x.to(cfg.device) , y.to(cfg.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1wsY5pufYA7"
      },
      "outputs": [],
      "source": [
        "class RMSNorm(nn.Module):\n",
        "  def __init__(self, d, eps = 1e-5):\n",
        "    super().__init__()\n",
        "    self.weight = nn.Parameter(torch.ones(d))\n",
        "    self.eps = eps\n",
        "\n",
        "  def forward(self,x):\n",
        "    norm = x.norm(dim = -1 ,keepdim = True)*(1.0/ math.sqrt(x.shape[-1]))\n",
        "    return self.weight * (x/ (norm + self.eps))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsjWY8zWhl5J"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "  def __init__(self,d_model,n_head,dropout = 0.0):\n",
        "    super().__init__()\n",
        "    assert d_model % n_head == 0\n",
        "    self.n_head = n_head\n",
        "    self.head_dim = d_model // n_head\n",
        "    self.qkv = nn.Linear(d_model , 3*d_model,bias = False)\n",
        "    self.proj = nn.Linear(d_model,d_model,bias = False)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    B,T,C = x.shape\n",
        "    qkv = self.qkv(x).view(B,T,3,self.n_head,self.head_dim).transpose(1,2)\n",
        "    q,k,v = qkv[:,0],qkv[:,1],qkv[:,2]\n",
        "    y = F.scaled_dot_product_attention(\n",
        "        q.transpose(1,2),k.transpose(1,2),v.transpose(1,2),attn_mask= None,\n",
        "    )\n",
        "    y = y.transpose(1,2).contiguous().view(B,T,C)\n",
        "    y = self.proj(y)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPNXzGYmkVaR"
      },
      "outputs": [],
      "source": [
        "class ExpertMLP(nn.Module):\n",
        "  def __init__(self, d_model, d_hidden):\n",
        "    super().__init__() # Add this line\n",
        "    self.fc1 = nn.Linear(d_model,d_hidden)\n",
        "    self.fc2 = nn.Linear(d_hidden,d_model)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.fc2(F.gelu(self.fc1(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoJTCD3ToHbm"
      },
      "outputs": [],
      "source": [
        "class Top1Router(nn.Module):\n",
        "  def __init__(self,d_model,n_experts = 5):\n",
        "    super().__init__() # Add this line\n",
        "    self.proj = nn.Linear(d_model,n_experts)\n",
        "    self.n_experts = n_experts\n",
        "\n",
        "  def forward(self,x):\n",
        "    B,T,D = x.shape\n",
        "    h = x.reshape(B*T,D)\n",
        "    logits = self.proj(h)\n",
        "    probs = F.softmax(logits,dim=-1)\n",
        "    top1 = probs.argmax(dim=-1)\n",
        "    w = probs.gather(1,top1.unsqueeze(-1)) # Ensure .squeeze(1) is removed\n",
        "    #load balance proxy\n",
        "    with torch.no_grad():\n",
        "      assign = F.one_hot(top1,num_classes= self.n_experts).float()\n",
        "    importance = probs.mean(dim=0)\n",
        "    load = assign.mean(dim=0)\n",
        "    lb_loss = self.n_experts * torch.sum(importance * load)\n",
        "    z_loss =(torch.logsumexp(logits, dim=-1)**2).mean()\n",
        "    return top1, w, lb_loss, z_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPUnm6gCtiqn"
      },
      "outputs": [],
      "source": [
        "class MoEMLP(nn.Module):\n",
        "    def __init__(self, d_model, d_hidden, n_experts, capacity_factor=1.25,\n",
        "                 lbl_coef=0.01, zloss_coef=0.0, top_k=1):\n",
        "        super().__init__()\n",
        "        assert top_k == 1, \"This minimal demo implements top-1 routing (Switch-style).\"\n",
        "        self.router = Top1Router(d_model, n_experts)\n",
        "        self.experts = nn.ModuleList([ExpertMLP(d_model, d_hidden) for _ in range(n_experts)])\n",
        "        self.n_experts = n_experts\n",
        "        self.capacity_factor = capacity_factor\n",
        "        self.lbl_coef = lbl_coef\n",
        "        self.zloss_coef = zloss_coef\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,D = x.shape\n",
        "        N = B*T\n",
        "        top1, w, lb_loss, z_loss = self.router(x)\n",
        "        cap = int(self.capacity_factor * (N / self.n_experts) + 1)\n",
        "\n",
        "        flat_x = x.reshape(N, D)\n",
        "        out = torch.zeros_like(flat_x)\n",
        "\n",
        "        for e in range(self.n_experts):\n",
        "            # Get indices of tokens routed to this expert, flattened\n",
        "            idx = (top1 == e).nonzero(as_tuple=False).flatten() # Flatten the indices\n",
        "            if idx.numel() == 0:\n",
        "                continue\n",
        "            if idx.numel() > cap:  # drop overflow\n",
        "                idx = idx[:cap]\n",
        "            xe = flat_x[idx]\n",
        "            ye = self.experts[e](xe)\n",
        "\n",
        "            we = w[idx].reshape(-1, 1) # Explicitly reshape to ensure shape is [num_tokens, 1]\n",
        "            out[idx] = ye * we # Assignment back to the flattened output tensor\n",
        "\n",
        "        y = out.reshape(B, T, D)\n",
        "        aux = self.lbl_coef * lb_loss + self.zloss_coef * z_loss\n",
        "        return y, aux"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJZiH1xrtnRm"
      },
      "outputs": [],
      "source": [
        "class DenseMLP(nn.Module):\n",
        "    def __init__(self, d_model, d_hidden):\n",
        "        super().__init__() # Add this line\n",
        "        self.ff = ExpertMLP(d_model, d_hidden)\n",
        "    def forward(self, x):\n",
        "        y = self.ff(x)\n",
        "        aux = x.new_tensor(0.0)\n",
        "        return y, aux"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11mEsxjVtqB1"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, d_model, n_head, d_mlp, use_moe, n_experts, capacity_factor, lbl_coef, zloss_coef):\n",
        "        super().__init__()\n",
        "        self.ln1 = RMSNorm(d_model)\n",
        "        self.attn = CausalSelfAttention(d_model, n_head)\n",
        "        self.ln2 = RMSNorm(d_model)\n",
        "        if use_moe:\n",
        "            self.mlp = MoEMLP(d_model, d_mlp, n_experts, capacity_factor, lbl_coef, zloss_coef, top_k=1)\n",
        "        else:\n",
        "            self.mlp = DenseMLP(d_model, d_mlp)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        y, aux = self.mlp(self.ln2(x))\n",
        "        x = x + y\n",
        "        return x, aux"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hG4iZlzPuxqs"
      },
      "outputs": [],
      "source": [
        "class TinyGPT(nn.Module):\n",
        "    def __init__(self, cfg, vocab_size):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.tok_emb = nn.Embedding(vocab_size, cfg.d_model)\n",
        "        self.pos_emb = nn.Parameter(torch.zeros(1, cfg.block_size, cfg.d_model))\n",
        "        self.drop = nn.Dropout(cfg.dropout)\n",
        "\n",
        "        blocks = []\n",
        "        for i in range(cfg.n_layer):\n",
        "            use_moe_this = cfg.use_moe and (i % 2 == 1)  # MoE every other block\n",
        "            blocks.append(Block(cfg.d_model, cfg.n_head, cfg.d_mlp, use_moe_this,\n",
        "                                cfg.n_experts, cfg.capacity_factor, cfg.load_balance_coef, cfg.zloss_coef))\n",
        "        self.blocks = nn.ModuleList(blocks)\n",
        "        self.ln_f = RMSNorm(cfg.d_model)\n",
        "        self.head = nn.Linear(cfg.d_model, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B,T = idx.shape\n",
        "        assert T <= self.cfg.block_size\n",
        "        x = self.tok_emb(idx) + self.pos_emb[:, :T, :]\n",
        "        x = self.drop(x)\n",
        "\n",
        "        aux_total = x.new_tensor(0.0)\n",
        "        for blk in self.blocks:\n",
        "            x, aux = blk(x)\n",
        "            aux_total = aux_total + aux\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(\n",
        "                logits.view(-1, logits.size(-1)), targets.view(-1)\n",
        "            )\n",
        "        return logits, loss, aux_total\n",
        "\n",
        "    def generate(self, idx, max_new_tokens=200, temperature=1.0, top_k=None):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -self.cfg.block_size:]\n",
        "            with torch.no_grad():\n",
        "                logits, _, _ = self.forward(idx_cond)\n",
        "                logits = logits[:, -1, :] / max(1e-6, temperature)\n",
        "                if top_k is not None:\n",
        "                    v, _ = torch.topk(logits, top_k)\n",
        "                    logits[logits < v[:, [-1]]] = -float('inf')\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                next_id = torch.multinomial(probs, num_samples=1)\n",
        "                idx = torch.cat((idx, next_id), dim=1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfobxN1Ju1zG"
      },
      "outputs": [],
      "source": [
        "model = TinyGPT(cfg, vocab_size).to(cfg.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWT_B-Ssu3YI",
        "outputId": "206e2d94-c55d-4940-f5cb-bcf4e77c8244"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TinyGPT(\n",
              "  (tok_emb): Embedding(65, 512)\n",
              "  (drop): Dropout(p=0.0, inplace=False)\n",
              "  (blocks): ModuleList(\n",
              "    (0): Block(\n",
              "      (ln1): RMSNorm()\n",
              "      (attn): CausalSelfAttention(\n",
              "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
              "        (proj): Linear(in_features=512, out_features=512, bias=False)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ln2): RMSNorm()\n",
              "      (mlp): DenseMLP(\n",
              "        (ff): ExpertMLP(\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): Block(\n",
              "      (ln1): RMSNorm()\n",
              "      (attn): CausalSelfAttention(\n",
              "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
              "        (proj): Linear(in_features=512, out_features=512, bias=False)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ln2): RMSNorm()\n",
              "      (mlp): MoEMLP(\n",
              "        (router): Top1Router(\n",
              "          (proj): Linear(in_features=512, out_features=4, bias=True)\n",
              "        )\n",
              "        (experts): ModuleList(\n",
              "          (0-3): 4 x ExpertMLP(\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): Block(\n",
              "      (ln1): RMSNorm()\n",
              "      (attn): CausalSelfAttention(\n",
              "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
              "        (proj): Linear(in_features=512, out_features=512, bias=False)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ln2): RMSNorm()\n",
              "      (mlp): DenseMLP(\n",
              "        (ff): ExpertMLP(\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (3): Block(\n",
              "      (ln1): RMSNorm()\n",
              "      (attn): CausalSelfAttention(\n",
              "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
              "        (proj): Linear(in_features=512, out_features=512, bias=False)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ln2): RMSNorm()\n",
              "      (mlp): MoEMLP(\n",
              "        (router): Top1Router(\n",
              "          (proj): Linear(in_features=512, out_features=4, bias=True)\n",
              "        )\n",
              "        (experts): ModuleList(\n",
              "          (0-3): 4 x ExpertMLP(\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (4): Block(\n",
              "      (ln1): RMSNorm()\n",
              "      (attn): CausalSelfAttention(\n",
              "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
              "        (proj): Linear(in_features=512, out_features=512, bias=False)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ln2): RMSNorm()\n",
              "      (mlp): DenseMLP(\n",
              "        (ff): ExpertMLP(\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (5): Block(\n",
              "      (ln1): RMSNorm()\n",
              "      (attn): CausalSelfAttention(\n",
              "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
              "        (proj): Linear(in_features=512, out_features=512, bias=False)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ln2): RMSNorm()\n",
              "      (mlp): MoEMLP(\n",
              "        (router): Top1Router(\n",
              "          (proj): Linear(in_features=512, out_features=4, bias=True)\n",
              "        )\n",
              "        (experts): ModuleList(\n",
              "          (0-3): 4 x ExpertMLP(\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (ln_f): RMSNorm()\n",
              "  (head): Linear(in_features=512, out_features=65, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 182,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LNSaxDyvIXL",
        "outputId": "e4940dc8-3268-4f94-eb8a-af459da32678"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3447504660.py:1: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled = (cfg.dtype == torch.float16))\n",
            "/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "scaler = torch.cuda.amp.GradScaler(enabled = (cfg.dtype == torch.float16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8WQuqAuvR9q"
      },
      "outputs": [],
      "source": [
        "model_dtype = torch.bfloat16 if cfg.dtype==torch.bfloat16 else torch.float16\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oMTf0MNvTK2"
      },
      "outputs": [],
      "source": [
        "model = model.to(dtype=model_dtype)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHwgcbTwvUfU"
      },
      "outputs": [],
      "source": [
        "\n",
        "if cfg.compile_model and hasattr(torch, \"compile\"):\n",
        "    model = torch.compile(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5DiRb5bvV6P"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr = cfg.lr,weight_decay=cfg.weight_decay,betas = (0.9,0.95))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFWB3ZDgvlP9",
        "outputId": "90918674-e2c4-4c48-ae4b-e040e705647b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AdamW (\n",
              "Parameter Group 0\n",
              "    amsgrad: False\n",
              "    betas: (0.9, 0.95)\n",
              "    capturable: False\n",
              "    decoupled_weight_decay: True\n",
              "    differentiable: False\n",
              "    eps: 1e-08\n",
              "    foreach: None\n",
              "    fused: None\n",
              "    lr: 0.0003\n",
              "    maximize: False\n",
              "    weight_decay: 0.1\n",
              ")"
            ]
          },
          "execution_count": 188,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-_bgnNNvoM3"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def count_params(m):\n",
        "    return sum(p.numel() for p in m.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-D-v7ZewKO6",
        "outputId": "ecdb7062-5277-4e40-8de1-e4dc32d0b3eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 65\n",
            "Total params: 38.00M\n",
            "Using MoE: True, n_experts=4 (active experts/token: 1)\n",
            "Device: cpu, dtype: torch.float16\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "print(f\"Vocab size: {vocab_size}\")\n",
        "print(f\"Total params: {count_params(model)/1e6:.2f}M\")\n",
        "print(f\"Using MoE: {cfg.use_moe}, n_experts={cfg.n_experts} (active experts/token: {1 if cfg.use_moe else 'N/A'})\")\n",
        "print(f\"Device: {cfg.device}, dtype: {model_dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-a-hTQhQwj8n"
      },
      "outputs": [],
      "source": [
        "def cosine_lr(step, max_steps, base_lr, warmup):\n",
        "    if step < warmup:\n",
        "        return base_lr * (step+1) / warmup\n",
        "    progress = (step - warmup) / max(1, (max_steps - warmup))\n",
        "    return base_lr * 0.5 * (1 + math.cos(math.pi * progress))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wd80ZPGhwrMY"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss(iters=20):\n",
        "    model.eval()\n",
        "    losses, auxes = [], []\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        ltot, atot = 0.0, 0.0\n",
        "        for _ in range(iters):\n",
        "            xb, yb = get_batch(split)\n",
        "            with torch.autocast(device_type=\"cuda\" if cfg.device==\"cuda\" else \"cpu\", dtype=model_dtype):\n",
        "                _, loss, aux = model(xb, yb)\n",
        "            ltot += loss.item()\n",
        "            atot += aux.item()\n",
        "        losses.append(ltot/iters)\n",
        "        auxes.append(atot/iters)\n",
        "    model.train()\n",
        "    return {\"train\": losses[0], \"val\": losses[1], \"aux_train\": auxes[0], \"aux_val\": auxes[1]}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gE48lXhwtOk",
        "outputId": "46c141e0-2e35-499d-8f8b-552d2b016611"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TinyGPT(\n",
              "  (tok_emb): Embedding(65, 512)\n",
              "  (drop): Dropout(p=0.0, inplace=False)\n",
              "  (blocks): ModuleList(\n",
              "    (0): Block(\n",
              "      (ln1): RMSNorm()\n",
              "      (attn): CausalSelfAttention(\n",
              "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
              "        (proj): Linear(in_features=512, out_features=512, bias=False)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ln2): RMSNorm()\n",
              "      (mlp): DenseMLP(\n",
              "        (ff): ExpertMLP(\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): Block(\n",
              "      (ln1): RMSNorm()\n",
              "      (attn): CausalSelfAttention(\n",
              "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
              "        (proj): Linear(in_features=512, out_features=512, bias=False)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ln2): RMSNorm()\n",
              "      (mlp): MoEMLP(\n",
              "        (router): Top1Router(\n",
              "          (proj): Linear(in_features=512, out_features=4, bias=True)\n",
              "        )\n",
              "        (experts): ModuleList(\n",
              "          (0-3): 4 x ExpertMLP(\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): Block(\n",
              "      (ln1): RMSNorm()\n",
              "      (attn): CausalSelfAttention(\n",
              "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
              "        (proj): Linear(in_features=512, out_features=512, bias=False)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ln2): RMSNorm()\n",
              "      (mlp): DenseMLP(\n",
              "        (ff): ExpertMLP(\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (3): Block(\n",
              "      (ln1): RMSNorm()\n",
              "      (attn): CausalSelfAttention(\n",
              "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
              "        (proj): Linear(in_features=512, out_features=512, bias=False)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ln2): RMSNorm()\n",
              "      (mlp): MoEMLP(\n",
              "        (router): Top1Router(\n",
              "          (proj): Linear(in_features=512, out_features=4, bias=True)\n",
              "        )\n",
              "        (experts): ModuleList(\n",
              "          (0-3): 4 x ExpertMLP(\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (4): Block(\n",
              "      (ln1): RMSNorm()\n",
              "      (attn): CausalSelfAttention(\n",
              "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
              "        (proj): Linear(in_features=512, out_features=512, bias=False)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ln2): RMSNorm()\n",
              "      (mlp): DenseMLP(\n",
              "        (ff): ExpertMLP(\n",
              "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (5): Block(\n",
              "      (ln1): RMSNorm()\n",
              "      (attn): CausalSelfAttention(\n",
              "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
              "        (proj): Linear(in_features=512, out_features=512, bias=False)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ln2): RMSNorm()\n",
              "      (mlp): MoEMLP(\n",
              "        (router): Top1Router(\n",
              "          (proj): Linear(in_features=512, out_features=4, bias=True)\n",
              "        )\n",
              "        (experts): ModuleList(\n",
              "          (0-3): 4 x ExpertMLP(\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (ln_f): RMSNorm()\n",
              "  (head): Linear(in_features=512, out_features=65, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 193,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1XCovjmwuuZ"
      },
      "outputs": [],
      "source": [
        "context = \"JULIET: \"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTvIB8xVwyTF"
      },
      "outputs": [],
      "source": [
        "ctx = torch.tensor([[stoi.get(c, 0) for c in context]], dtype=torch.long, device=cfg.device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "40zYVoYOwzzA"
      },
      "outputs": [],
      "source": [
        "out = model.generate(ctx, max_new_tokens=300, temperature=0.8, top_k=50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBuqBWCdw1EX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09d02180"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}